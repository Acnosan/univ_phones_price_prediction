{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import Lasso,Ridge\n",
    "from sklearn.model_selection import KFold,GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"mobile phone price prediction.csv\"\n",
    "random_state = 29\n",
    "K_cv = [8]\n",
    "alpha_values = [0.01,0.05,0.1,0.2,0.5,1,2,5,10,11,12,13,14,15,16,17,18,19,20,30,40,50]\n",
    "max_iterations = [10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000]\n",
    "tolerance = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "\n",
    "models = {\n",
    "    \"Lasso\":Lasso(), # L1\n",
    "    \"Ridge\":Ridge() # L2\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"Lasso\": {\n",
    "        \"model__alpha\": alpha_values,\n",
    "        \"model__max_iter\": max_iterations,\n",
    "        \"model__tol\": tolerance\n",
    "    },\n",
    "    \"Ridge\": {\n",
    "        \"model__alpha\": alpha_values,\n",
    "        \"model__max_iter\": max_iterations,\n",
    "        \"model__solver\": [\"auto\", \"saga\"],\n",
    "        \"model__tol\": tolerance\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original = pd.read_csv(dataset_path)\n",
    "length_dataset = len(dataset_original)\n",
    "#print(dataset_original.info())\n",
    "#print(dataset_original.head(3))\n",
    "\n",
    "# renaming so all columns start with Capital letter\n",
    "dataset_original.rename(columns={'Processor': 'Processor_core','company':'Company','fast_charging':'Fast_charging'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHOSING WHICH COLUMNS ARE THE BEST FOR OUR PRICE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"Processor_core\" , \"Company\" , \"Processor_name\"]\n",
    "numerical_columns = [\"Ram\" , \"Battery\" , \"Display\" , \"Android_version\" , \"Inbuilt_memory\" , \"Fast_charging\", \"Camera\"]\n",
    "target_column = [\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove outliers using our knowledge \n",
    "# it means for example RAM column should have values between 2 and 16 for example , LIKE WE KNOW THIS IN REAL LIFE ( OUR REAL LIFE KNOWLEDGE )\n",
    "def remove_outliers_with_domain_knowledge(df, lower_bound, upper_bound):\n",
    "    \"\"\"\n",
    "    Removing outliers with our real life knowledge on the topic\n",
    "    \n",
    "    Args:\n",
    "        df: The Target Dataframe.\n",
    "        lower_bound: Lowest values for our features.\n",
    "        upper_bound: Highest values for our features.\n",
    "    \n",
    "    Returns:\n",
    "        A Dataframe with clipped borders thus no extreme values, without the outliers.\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    for idx, col in enumerate(numerical_columns + target_column):\n",
    "        #print(f\"Before: Min = {df[col].min()}, Max = {df[col].max()}\")\n",
    "        # REMOVING VALUES THAT ARE LOWER OR BIGGER THE THE BOUNDS\n",
    "        df_cleaned[col] = df[col].clip(lower=lower_bound[idx], upper=upper_bound[idx])\n",
    "        #print(f\"After: Min = {df_cleaned[col].min()}, Max = {df_cleaned[col].max()}\")\n",
    "        \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_visualization(df,state):\n",
    "    \"\"\"\n",
    "    Box plot visualization \n",
    "    \n",
    "    Args:\n",
    "        df: The Target Dataframe.\n",
    "        state: State of our df, 'before or after'\n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    columns = numerical_columns + target_column\n",
    "\n",
    "    # Set the number of columns for each row (in this case, 2)\n",
    "    num_columns = 2\n",
    "    num_rows = (len(columns) + 1) // num_columns  # Calculate the number of rows needed\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, 5 * num_rows))\n",
    "    \n",
    "    # Flatten the axes array for easier iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each histogram on the corresponding subplot\n",
    "    for idx, col in enumerate(columns):\n",
    "        sns.boxplot(x=df[col],ax=axes[idx])\n",
    "        axes[idx].set_title(f\"{col} State: {state} Boxplot\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot_visualization(df):\n",
    "    \"\"\"\n",
    "    Histograme plot visualization \n",
    "    \n",
    "    Args:\n",
    "        df: The Target Dataframe.\n",
    "\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    # Combine numerical columns and target column\n",
    "    columns = numerical_columns + target_column\n",
    "\n",
    "    # Set the number of columns for each row (in this case, 2)\n",
    "    num_columns = 2\n",
    "    num_rows = (len(columns) + 1) // num_columns  # Calculate the number of rows needed\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, 5 * num_rows))\n",
    "    \n",
    "    # Flatten the axes array for easier iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot each histogram on the corresponding subplot\n",
    "    for idx, col in enumerate(columns):\n",
    "        sns.histplot(df[col], kde=True, bins=30, ax=axes[idx])\n",
    "        axes[idx].set_title(f\"{col} : Capacity Distribution\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_visualization(df):\n",
    "    \"\"\"\n",
    "    Heatmap, Correlation plot visualization \n",
    "    \n",
    "    Args:\n",
    "        df: The Target Dataframe.\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    correlation_matrix = df[numerical_columns+target_column].corr()\n",
    "    # Plot the correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_visualization(X,y):\n",
    "    \"\"\"\n",
    "    Feature importance visualization\n",
    "    \n",
    "    Args:\n",
    "        X: Our Features.\n",
    "        y: Target Feature.\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    importance = mutual_info_regression(X, y.values.ravel())\n",
    "\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importance\n",
    "    })\n",
    "\n",
    "    # Sort features by importance\n",
    "    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(\n",
    "        data=feature_importance, \n",
    "        x='Importance', \n",
    "        y='Feature', \n",
    "        hue='Feature',  # Assign the `y` variable (Feature) to `hue`\n",
    "        dodge=False,    # Ensure no separation by hue\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title('Feature Importance Based on Mutual Information')\n",
    "    plt.xlabel('Mutual Information Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_with_gridsearch(X, y, models, param_grids):\n",
    "    \"\"\"\n",
    "    Train and evaluate models using GridSearchCV for hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        X: Features dataframe.\n",
    "        y: Target dataframe.\n",
    "        models: Dictionary of model names and their initialized estimators.\n",
    "        param_grids: Dictionary of model names and their hyperparameter grids.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame with results for each model and hyperparameter combination.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        for k_split in K_cv:\n",
    "            param_grid = param_grids[name]\n",
    "            \n",
    "            # Create a pipeline to include scaling\n",
    "            pipeline = Pipeline([\n",
    "                (\"scaler\", feature_scaler),\n",
    "                (\"model\", model)\n",
    "            ])\n",
    "            \n",
    "            # GridSearchCV setup\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_grid=param_grid,\n",
    "                scoring={\"mse\": \"neg_mean_squared_error\", \"r2\": \"r2\"},  # Multiple scoring metrics\n",
    "                refit=\"r2\",  # Choose the best model based on R2\n",
    "                cv=KFold(n_splits=k_split, shuffle=True, random_state=random_state),  # Cross-validation splits\n",
    "                n_jobs=-1  # Parallelize for faster execution\n",
    "            )\n",
    "            # Train GridSearchCV\n",
    "            grid_search.fit(X, y)\n",
    "            \n",
    "            results.append({\n",
    "                \"model_name\": name,\n",
    "                \"model\": grid_search.best_estimator_,\n",
    "                \"K_cv\": k_split,\n",
    "                \"best_params\": grid_search.best_params_,\n",
    "                \"best_mse\": -grid_search.best_score_,  # Negate since MSE is negative\n",
    "                \"best_r2\": grid_search.cv_results_[\"mean_test_r2\"][grid_search.best_index_],\n",
    "                \"train_time\": grid_search.refit_time_\n",
    "            })\n",
    "            \n",
    "    #fitted_scaler = grid_search.best_estimator_.named_steps['scaler']\n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(input_data, real_prices, model, X, lower_bound, upper_bound):\n",
    "    \"\"\"\n",
    "    Train and evaluate models using GridSearchCV for hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        input_data: Input Dataframe.\n",
    "        real_prices: Real prices list that corresponds to our input_data.\n",
    "        model: Dictionary of best model name and it initialized estimators.\n",
    "        X: Features dataframe.\n",
    "        lower_bound: Lowest values for our features.\n",
    "        upper_bound: Highest values for our features.\n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    input_data_encoded = pd.get_dummies(input_data, columns=categorical_columns)\n",
    "    input_data_encoded = input_data_encoded.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "    prediction = model.predict(input_data_encoded)\n",
    "    # Clip to bounds\n",
    "    prediction = np.clip(prediction, lower_bound[-1], upper_bound[-1])\n",
    "\n",
    "    for idx, pred in enumerate(prediction):\n",
    "        #print(f\"Type of prediction: {type(pred)}, Value: {pred}\")\n",
    "        print(f\"The predicted price of the phone is: {pred.item():.2f}\")\n",
    "        print(f\"Difference in price: {int(pred.item()) - real_prices[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,want_save):\n",
    "    \"\"\"\n",
    "    Saving Model Based on Condition.\n",
    "    \n",
    "    Args:\n",
    "        model: Dictionary of best model name and it initialized estimators.\n",
    "        want_save : Boolean to confirm the saving process.\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    if not want_save:\n",
    "        return\n",
    "    model_filename = f\"{model['model_name']}_model_k{model['K_cv']}.joblib\"\n",
    "    joblib.dump(model['model'], model_filename)\n",
    "    print(f\"Model saved as: {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROPING COLUMNS THAT ARE NOT SUITED FOR OUR PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset_original.columns:\n",
    "    if col not in categorical_columns+numerical_columns+target_column:\n",
    "        dataset_original.drop(col,axis=1,inplace=True)\n",
    "        \n",
    "dataset_original.drop_duplicates(inplace=True)\n",
    "dataset_original.dropna(axis=0,how='all',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot_visualization(dataset_original,'before')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STANDARIZING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HERE I CHOSE WHICH DATA TO KEEP AND TO REPLACE BASED ON .unique() method , i didnt want to include it here cause of too much print() statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR CATEGORICAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original[\"Processor_core\"] = dataset_original[\"Processor_core\"].str.strip()\n",
    "dataset_original.loc[dataset_original[\"Processor_core\"].str.contains(r'\\d+\\.?\\d*',regex=True,na=False), \"Processor_core\"] = np.nan\n",
    "dataset_original[\"Processor_core\"].replace({\n",
    "    'Octa Core Processor': 'Octa Core',\n",
    "    'Nine-Cores': 'Nine Core',\n",
    "    'Nine Cores': 'Nine Core',\n",
    "    'Deca Core Processor': 'Deca Core'\n",
    "},regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original[\"Company\"] = dataset_original[\"Company\"].astype(str)\n",
    "dataset_original[\"Company\"].replace({\"Nothing\":np.nan},regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original[\"Processor_name\"] = dataset_original[\"Processor_name\"].astype(str)\n",
    "\n",
    "dataset_original.loc[dataset_original[\"Processor_name\"].str.contains(r'Core|Wifi|GB', na=False, case=False,regex=True), \"Processor_name\"] = np.nan\n",
    "dataset_original.loc[dataset_original[\"Processor_name\"].str.contains(\"Samsung\", na=False, case=False), \"Processor_name\"] = \"Exynos\"\n",
    "dataset_original.loc[dataset_original[\"Processor_name\"].str.contains(r\"Sanpdragon|Snapdragon\",regex=True,na=False, case=False), \"Processor_name\"] = \"Snapdragon\"\n",
    "dataset_original.loc[dataset_original[\"Processor_name\"].str.contains(\"Dimensity\", na=False, case=False), \"Processor_name\"] = \"Dimensity\"\n",
    "\n",
    "dataset_original[\"Processor_name\"].replace(r'\\s+\\d+\\w*|\\s+\\w+',\"\",regex=True,inplace=True)\n",
    "dataset_original[\"Processor_name\"].replace(\"\",np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILLING MISSING VALUES WITH MODE FOR CATEGORICAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    dataset_original[col] = dataset_original[col].str.strip().str.lower()\n",
    "    print(f\"the ratio of nan {col}: {dataset_original[col].isna().sum()*100 / len(dataset_original):.2f}\")\n",
    "    dataset_original[col].fillna(dataset_original[col].mode()[0],inplace=True)\n",
    "    print(dataset_original[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONE HOT ENCODING FOR OUR CATEGORICAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original = pd.get_dummies(dataset_original, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR NUMERICAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original[\"Ram\"].replace(r\"\\s*GB\\s*RAM\", \"\",regex=True,inplace=True)\n",
    "dataset_original[\"Battery\"].replace(r\"\\s*mAh\\s*Battery\", \"\",regex=True,inplace=True)\n",
    "dataset_original[\"Display\"].replace(r\"\\s*inches\", \"\",regex=True,inplace=True)\n",
    "dataset_original[\"Inbuilt_memory\"].replace(r\"\\s*GB\\s*inbuilt\", \"\",regex=True,inplace=True)\n",
    "\n",
    "dataset_original[\"Fast_charging\"] = dataset_original[\"Fast_charging\"].str.extract(r\"(\\d+\\.?\\d*)\")\n",
    "\n",
    "dataset_original[\"Camera\"] = dataset_original[\"Camera\"].str.strip().str.lower()\n",
    "dataset_original.loc[dataset_original[\"Camera\"].str.contains(r'display|memory', case=False, na=False),'Camera'] = np.nan\n",
    "dataset_original[\"Camera_rear\"] = dataset_original[\"Camera\"].str.extract(r\"(\\d+)\\s*mp\")\n",
    "dataset_original[\"Camera_front\"] = dataset_original[\"Camera\"].str.extract(r\";\\s*(\\d+)\\s*mp\")\n",
    "dataset_original.drop(\"Camera\",axis=1,inplace=True)\n",
    "\n",
    "dataset_original[\"Price\"].replace(',','', regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we remove camera column and change it to new 2 columns ( camera rear and camera front )\n",
    "numerical_columns.pop()\n",
    "numerical_columns.extend([\"Camera_rear\", \"Camera_front\"])\n",
    "# regressor columns are columns that we will fill using another model predictions cause these columns have NAN values ratio > 5-30%\n",
    "# so its better then using mean or median\n",
    "regressor_columns = [\"Android_version\",\"Fast_charging\",\"Camera_rear\", \"Camera_front\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### casting the numerical columns to be float using pandas.to_numeric and removing the nan values except for regressor columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_columns+target_column:\n",
    "    dataset_original[col] = pd.to_numeric(dataset_original[col],errors='coerce')\n",
    "    print(f\"the ratio of nan {col}: {dataset_original[col].isna().sum()*100 / len(dataset_original):.2f}\")\n",
    "    if col not in regressor_columns:\n",
    "        dataset_original[col].fillna(dataset_original[col].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING OUTLIERS AFTER GOT DETECTED USING BOX PLOT WITH OUR KNOWLEDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE THE COLUMNS ORDER IS :\n",
    "# Ram , Battery , Display , Android_version , Inbuilt_memory , Fast_charging , Camera_rear, Camera_front , Price\n",
    "# for example here LOWER BOUND : lowest ram = 2 , lowest battery = 2500 , lowest display = 5.5 ETC\n",
    "# SAME FOR UPPER BOUND                                 \n",
    "lower_bound = [2,2500,5.5,8,16,10,5,2,6950]\n",
    "upper_bound = [16,6500,7.5,14,256,140,80,60,70000]\n",
    "\n",
    "dataset_cleaned_encoded = remove_outliers_with_domain_knowledge(dataset_original, lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILLING NAN VALUES FOR THE REGRESSOR COLUMNS WITH ANOTHER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING A DATASET FOR ALL COLUMNS EXCEPT THE TARGET COLUMN ( PRICE ) , ITS BETTER SO WE DONT OVERRIDE OUR ORIGINAL DATASET\n",
    "dataset_for_regressor = dataset_cleaned_encoded[[col for col in dataset_cleaned_encoded.columns if col not in target_column]].copy()\n",
    "# we use random forest regressor to fill nan values here\n",
    "regressor = RandomForestRegressor(n_estimators=100,random_state=random_state)\n",
    "\n",
    "for col in regressor_columns:\n",
    "    # making train dataset using rows that have NOT NULL values in the regressor column \n",
    "    # meaning for example all rows where android_version is not null\n",
    "    train_dataset_for_regressor = dataset_for_regressor[dataset_for_regressor[col].notna()]\n",
    "    # making test dataset using rows that have NULL values in the regressor column\n",
    "    test_dataset_for_regressor = dataset_for_regressor[dataset_for_regressor[col].isna()]\n",
    "\n",
    "    # we drop the column that we want to predict FROM THE DATASET WHERE col is NOT NULL , means all rows where COL was NOT NULL SO WE COULD MAKE PREDICTIONS\n",
    "    X_train = train_dataset_for_regressor.drop(columns=col)\n",
    "    # y is the column that we want to predict FROM THE DATASET WHERE col is NOT NULL , so we could make predictions \n",
    "    y_train = train_dataset_for_regressor[col]\n",
    "    # we drop the column that we want to predict FROM THE DATASET WHERE col is NULL , means all rows where COL was NULL\n",
    "    X_test = test_dataset_for_regressor.drop(columns=col)\n",
    "\n",
    "    regressor.fit(X_train,y_train)\n",
    "    print(f\"prediction for column {col} is done !\")\n",
    "    # FILLING THE COLUMNS WHERE ROW OF col IS NULL WITH THE RESULT OF THE REGRESSOR PRECITION ON THE TEST DATA\n",
    "    dataset_cleaned_encoded.loc[dataset_cleaned_encoded[col].isna(),col] = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THUS WE HAVE NOW \" DATASET_CLEANED_ENCODED \" READY FOR ALL PREDICTIONS !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist_plot_visualization(dataset_cleaned_encoded)\n",
    "#box_plot_visualization(dataset_cleaned_encoded,'after')\n",
    "#heatmap_visualization(dataset_cleaned_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_cleaned_encoded.drop(target_column,axis=1)\n",
    "y = dataset_cleaned_encoded.loc[:,target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = train_evaluate_with_gridsearch(X, y, models, param_grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_final_model = results_df.loc[results_df['best_mse'].idxmin()]\n",
    "print(f\"Best model is : {best_final_model['model_name']}\")\n",
    "print(f\"K folds : {best_final_model['K_cv']}\")   \n",
    "print(f\"Best Params: {best_final_model['best_params']}\")\n",
    "print(f\"Best MSE: {best_final_model['best_mse']:.2f}\")\n",
    "print(f\"Best R2: {best_final_model['best_r2']:.4f}\")\n",
    "print(f\"Training Time: {best_final_model['train_time']:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(best_final_model,'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data =pd.DataFrame({\n",
    "    'Ram': [4,6,6],\n",
    "    'Battery': [5100,5000,6000],\n",
    "    'Display': [6.67,6.71,6.5],\n",
    "    'Android_version': [14,12,14],\n",
    "    'Inbuilt_memory': [128,128,128],\n",
    "    'Fast_charging': [45,15,25],\n",
    "    'Camera_rear': [8,50,50],\n",
    "    'Camera_front': [5,5,13],\n",
    "    'Processor_core': [\"octa core\",\"octa core\",\"octa core\"],\n",
    "    'Company': [\"oppo\",\"poco\",\"samsung\"],\n",
    "    'Processor_name': [\"dimensity\",\"helio\",\"dimensity\"]\n",
    "    })\n",
    "\n",
    "real_prices = [13500,11000,14500]\n",
    "load_model_path = 'Ridge_model_k8.joblib'\n",
    "best_model = joblib.load(load_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_price(input_data, real_prices, best_model, X, lower_bound, upper_bound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
