{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,PolynomialFeatures\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"mobile phone price prediction.csv\"\n",
    "random_state = 29\n",
    "K_cv = [8]\n",
    "polynom_degree = [2,3]\n",
    "alpha_values = [0.01,0.05,0.1,0.2,0.5,1,2,5,10,11,12,13,14,15,18,20]\n",
    "max_iterations = [10000, 20000, 30000, 40000, 50000]\n",
    "tolerance = [1, 1e-1, 1e-2, 1e-3]\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "polynomial_feature_scaler = PolynomialFeatures()\n",
    "\n",
    "models = {\n",
    "    \"Simple\":LinearRegression(), # Linear Regression\n",
    "    \"Poly\":LinearRegression(), # Poly Regression\n",
    "    \"Lasso\":Lasso(), # L1 Regression\n",
    "    \"Ridge\":Ridge() # L2 Regression\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"Simple\": {\n",
    "        \"model__fit_intercept\": [True, False],\n",
    "    },\n",
    "    \"Poly\": {\n",
    "        \"poly__degree\": polynom_degree,\n",
    "        \"model__fit_intercept\": [True, False], \n",
    "    },\n",
    "    \"Lasso\": {\n",
    "        \"model__alpha\": alpha_values,\n",
    "        \"model__max_iter\": max_iterations,\n",
    "        \"model__tol\": tolerance\n",
    "    },\n",
    "    \"Ridge\": {\n",
    "        \"model__alpha\": alpha_values,\n",
    "        \"model__max_iter\": max_iterations,\n",
    "        \"model__solver\": [\"auto\", \"saga\"],\n",
    "        \"model__tol\": tolerance\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original = pd.read_csv(dataset_path)\n",
    "length_dataset = len(dataset_original)\n",
    "#print(dataset_original.info())\n",
    "#print(dataset_original.head(3))\n",
    "\n",
    "# renaming so all columns start with Capital letter\n",
    "dataset_original.rename(columns={'Processor': 'Processor_core','company':'Company','fast_charging':'Fast_charging'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHOSING WHICH COLUMNS ARE THE BEST FOR OUR PRICE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"Processor_core\" , \"Company\" , \"Processor_name\"]\n",
    "numerical_columns = [\"Ram\" , \"Battery\" , \"Display\" , \"Android_version\" , \"Inbuilt_memory\" , \"Fast_charging\", \"Camera\"]\n",
    "target_column = [\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove outliers using our knowledge \n",
    "# it means for example RAM column should have values between 2 and 16 for example , LIKE WE KNOW THIS IN REAL LIFE ( OUR REAL LIFE KNOWLEDGE )\n",
    "def remove_outliers_with_domain_knowledge(df, lower_bound, upper_bound):\n",
    "    \"\"\"\n",
    "    Removing outliers with our real life knowledge on the topic\n",
    "    \n",
    "    Args:\n",
    "        df: The Target Dataframe.\n",
    "        lower_bound: Lowest values for our features.\n",
    "        upper_bound: Highest values for our features.\n",
    "    \n",
    "    Returns:\n",
    "        A Dataframe with clipped borders thus no extreme values, without the outliers.\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    for idx, col in enumerate(numerical_columns + target_column):\n",
    "        #print(f\"Before: Min = {df[col].min()}, Max = {df[col].max()}\")\n",
    "        # REMOVING VALUES THAT ARE LOWER OR BIGGER THE THE BOUNDS\n",
    "        df_cleaned[col] = df[col].clip(lower=lower_bound[idx], upper=upper_bound[idx])\n",
    "        #print(f\"After: Min = {df_cleaned[col].min()}, Max = {df_cleaned[col].max()}\")\n",
    "        \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_visualization(df,state):\n",
    "    \"\"\"\n",
    "    Box plot visualization \n",
    "    \n",
    "    Args:\n",
    "        df: The Target Dataframe.\n",
    "        state: State of our df, 'before or after'\n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    columns = numerical_columns + target_column\n",
    "\n",
    "    # Set the number of columns for each row (in this case, 2)\n",
    "    num_columns = 2\n",
    "    num_rows = (len(columns) + 1) // num_columns  # Calculate the number of rows needed\n",
    "\n",
    "    # Create subplots\n",
    "    _, axes = plt.subplots(num_rows, num_columns, figsize=(15, 5 * num_rows))\n",
    "    \n",
    "    # Flatten the axes array for easier iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each histogram on the corresponding subplot\n",
    "    for idx, col in enumerate(columns):\n",
    "        sns.boxplot(x=df[col],ax=axes[idx])\n",
    "        axes[idx].set_title(f\"{col} State: {state} Boxplot\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot_visualization(df):\n",
    "    \"\"\"\n",
    "    Histograme plot visualization \n",
    "    \n",
    "    Args:\n",
    "        df: The Target Dataframe.\n",
    "\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    # Combine numerical columns and target column\n",
    "    columns = numerical_columns + target_column\n",
    "\n",
    "    # Set the number of columns for each row (in this case, 2)\n",
    "    num_columns = 2\n",
    "    num_rows = (len(columns) + 1) // num_columns  # Calculate the number of rows needed\n",
    "\n",
    "    # Create subplots\n",
    "    _, axes = plt.subplots(num_rows, num_columns, figsize=(15, 5 * num_rows))\n",
    "    \n",
    "    # Flatten the axes array for easier iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot each histogram on the corresponding subplot\n",
    "    for idx, col in enumerate(columns):\n",
    "        sns.histplot(df[col], kde=True, bins=30, ax=axes[idx])\n",
    "        axes[idx].set_title(f\"{col} : Capacity Distribution\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_visualization(df):\n",
    "    \"\"\"\n",
    "    Heatmap, Correlation plot visualization \n",
    "    \n",
    "    Args:\n",
    "        df: The Target Dataframe.\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    correlation_matrix = df[numerical_columns+target_column].corr()\n",
    "    # Plot the correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_visualization(X,y):\n",
    "    \"\"\"\n",
    "    Feature importance visualization\n",
    "    \n",
    "    Args:\n",
    "        X: Our Features.\n",
    "        y: Target Feature.\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    importance = mutual_info_regression(X, y.values.ravel())\n",
    "\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importance\n",
    "    })\n",
    "\n",
    "    # Sort features by importance\n",
    "    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(\n",
    "        data=feature_importance, \n",
    "        x='Importance', \n",
    "        y='Feature', \n",
    "        hue='Feature',  # Assign the `y` variable (Feature) to `hue`\n",
    "        dodge=False,    # Ensure no separation by hue\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title('Feature Importance Based on Mutual Information')\n",
    "    plt.xlabel('Mutual Information Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_with_gridsearch(X, y, models, param_grids):\n",
    "    \"\"\"\n",
    "    Train and evaluate models using GridSearchCV for hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        X: Features dataframe.\n",
    "        y: Target dataframe.\n",
    "        models: Dictionary of model names and their initialized estimators.\n",
    "        param_grids: Dictionary of model names and their hyperparameter grids.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame with results for each model and hyperparameter combination.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    scoring = {\n",
    "        'r2': 'r2',\n",
    "        'neg_mean_squared_error': 'neg_mean_squared_error',\n",
    "        'neg_mean_absolute_error': 'neg_mean_absolute_error'\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        for k_split in K_cv:\n",
    "            param_grid = param_grids[name]\n",
    "            # Create the preprocessing pipeline to include features scaling\n",
    "            if 'Poly' in name:\n",
    "                pipeline = Pipeline([\n",
    "                (\"poly\", polynomial_feature_scaler),\n",
    "                (\"scaler\", feature_scaler),\n",
    "                (\"model\", model)\n",
    "                ])\n",
    "            else:\n",
    "                pipeline = Pipeline([\n",
    "                    (\"scaler\", feature_scaler),\n",
    "                    (\"model\", model)\n",
    "                ])\n",
    "            \n",
    "            # GridSearchCV setup\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_grid=param_grid,\n",
    "                scoring=scoring,  # Multiple scoring metrics\n",
    "                refit=\"r2\",  # Choose the best model based on R2\n",
    "                cv=k_split,\n",
    "                n_jobs=-1  # Parallelize for faster execution\n",
    "            )\n",
    "\n",
    "            begin_train_time = time.time()\n",
    "            # Train GridSearchCV\n",
    "            grid_search.fit(X, y)\n",
    "            end_train_time = time.time()\n",
    "            \n",
    "            y_pred = cross_val_predict(grid_search.best_estimator_, X, y, cv=k_split)\n",
    "            y = np.array(y).ravel() ## making y to 1-d array to match with y_pred shape\n",
    "            \n",
    "            # Calculating the metrics :\n",
    "            y_mean = np.mean(y)\n",
    "            mse_calculated = np.mean((y - y_pred) ** 2) # mse = 1/n * sum( (y_real - y_predicted)**2 )\n",
    "            # note : the mse will be so big cause of the y values here ( prices ) are big numbers.\n",
    "            mape_calculated = np.mean(np.abs((y - y_pred) / y)) # mape = 1/n * sum( abs(y_real - y_predicted) )\n",
    "            \n",
    "            sce = np.sum((y_pred - y_mean) ** 2)\n",
    "            sst = np.sum((y - y_mean) ** 2)\n",
    "            r2_calculated = sce / sst # r2 = SCE / SST\n",
    "            \n",
    "            results.append({\n",
    "                \"model_name\": name,\n",
    "                \"model\": grid_search.best_estimator_,\n",
    "                \"K_cv\": k_split,\n",
    "                \"best_params\": grid_search.best_params_,\n",
    "                \"train_time\": end_train_time - begin_train_time,\n",
    "                \"mse_calculated\":mse_calculated,\n",
    "                \"mape_calculated\":mape_calculated,\n",
    "                \"r2_calculated\":r2_calculated\n",
    "            })\n",
    "            \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(input_data, real_prices, model, X, lower_bound, upper_bound):\n",
    "    \"\"\"\n",
    "    Train and evaluate models using GridSearchCV for hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        input_data: Input Dataframe.\n",
    "        real_prices: Real prices list that corresponds to our input_data.\n",
    "        model: Dictionary of best model name and it initialized estimators.\n",
    "        X: Features dataframe.\n",
    "        lower_bound: Lowest values for our features.\n",
    "        upper_bound: Highest values for our features.\n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    input_data_encoded = pd.get_dummies(input_data, columns=categorical_columns)\n",
    "    input_data_encoded = input_data_encoded.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "    prediction = model.predict(input_data_encoded)\n",
    "    # Clip to bounds in case of bad predictions\n",
    "    prediction = np.clip(prediction, lower_bound[-1], upper_bound[-1])\n",
    "\n",
    "    for idx, pred in enumerate(prediction):\n",
    "        #print(f\"Type of prediction: {type(pred)}, Value: {pred}\")\n",
    "        print(f\"The predicted price of the phone is: {pred.item():.2f}\")\n",
    "        print(f\"Difference in price: {int(pred.item()) - real_prices[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,want_save):\n",
    "    \"\"\"\n",
    "    Saving Model Based on Condition.\n",
    "    \n",
    "    Args:\n",
    "        model: Dictionary of best model name and it initialized estimators.\n",
    "        want_save : Boolean to confirm the saving process.\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    if not want_save:\n",
    "        return\n",
    "    model_filename = f\"{model['model_name']}_model_k{model['K_cv']}_r2_{model['mean_test_r2']:.3f}.joblib\"\n",
    "    joblib.dump(model['model'], model_filename)\n",
    "    print(f\"Model saved as: {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROPING COLUMNS THAT ARE NOT SUITED FOR OUR PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset_original.columns:\n",
    "    if col not in categorical_columns+numerical_columns+target_column:\n",
    "        dataset_original.drop(col,axis=1,inplace=True)\n",
    "        \n",
    "dataset_original.drop_duplicates(inplace=True)\n",
    "dataset_original.dropna(axis=0,how='all',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot_visualization(dataset_original,'before')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR CATEGORICAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original[\"Processor_core\"] = dataset_original[\"Processor_core\"].str.strip()\n",
    "dataset_original.loc[dataset_original[\"Processor_core\"].str.contains(r'\\d+\\.?\\d*',regex=True,na=False), \"Processor_core\"] = np.nan\n",
    "dataset_original[\"Processor_core\"].replace({\n",
    "    'Octa Core Processor': 'Octa Core',\n",
    "    'Nine-Cores': 'Nine Core',\n",
    "    'Nine Cores': 'Nine Core',\n",
    "    'Deca Core Processor': 'Deca Core'\n",
    "},regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original[\"Company\"] = dataset_original[\"Company\"].astype(str)\n",
    "dataset_original[\"Company\"].replace(\"Nothing\",np.nan,regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original[\"Processor_name\"] = dataset_original[\"Processor_name\"].astype(str)\n",
    "\n",
    "dataset_original.loc[dataset_original[\"Processor_name\"].str.contains(r'Core|Wifi|GB', na=False, case=False,regex=True), \"Processor_name\"] = np.nan\n",
    "dataset_original.loc[dataset_original[\"Processor_name\"].str.contains(\"Samsung\", na=False, case=False), \"Processor_name\"] = \"Exynos\"\n",
    "dataset_original.loc[dataset_original[\"Processor_name\"].str.contains(r\"Sanpdragon|Snapdragon\",regex=True,na=False, case=False), \"Processor_name\"] = \"Snapdragon\"\n",
    "dataset_original.loc[dataset_original[\"Processor_name\"].str.contains(\"Dimensity\", na=False, case=False), \"Processor_name\"] = \"Dimensity\"\n",
    "\n",
    "dataset_original[\"Processor_name\"].replace(r'\\s+\\d+\\w*|\\s+\\w+',\"\",regex=True,inplace=True)\n",
    "dataset_original[\"Processor_name\"].replace(\"\",np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILLING MISSING VALUES WITH MODE FOR CATEGORICAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    dataset_original[col] = dataset_original[col].str.strip().str.lower()\n",
    "    print(f\"the ratio of nan {col}: {dataset_original[col].isna().sum()*100 / len(dataset_original):.2f}\")\n",
    "    dataset_original[col].fillna(dataset_original[col].mode()[0],inplace=True)\n",
    "    print(dataset_original[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONE HOT ENCODING FOR OUR CATEGORICAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original = pd.get_dummies(dataset_original, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR NUMERICAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original[\"Ram\"].replace(r\"\\s*GB\\s*RAM\", \"\",regex=True,inplace=True)\n",
    "dataset_original[\"Battery\"].replace(r\"\\s*mAh\\s*Battery\", \"\",regex=True,inplace=True)\n",
    "dataset_original[\"Display\"].replace(r\"\\s*inches\", \"\",regex=True,inplace=True)\n",
    "dataset_original[\"Inbuilt_memory\"].replace(r\"\\s*GB\\s*inbuilt\", \"\",regex=True,inplace=True)\n",
    "\n",
    "dataset_original[\"Fast_charging\"] = dataset_original[\"Fast_charging\"].str.extract(r\"(\\d+\\.?\\d*)\")\n",
    "\n",
    "dataset_original[\"Camera\"] = dataset_original[\"Camera\"].str.strip().str.lower()\n",
    "dataset_original.loc[dataset_original[\"Camera\"].str.contains(r'display|memory', case=False, na=False),'Camera'] = np.nan\n",
    "dataset_original[\"Camera_rear\"] = dataset_original[\"Camera\"].str.extract(r\"(\\d+)\\s*mp\")\n",
    "dataset_original[\"Camera_front\"] = dataset_original[\"Camera\"].str.extract(r\";\\s*(\\d+)\\s*mp\")\n",
    "dataset_original.drop(\"Camera\",axis=1,inplace=True)\n",
    "\n",
    "dataset_original[\"Price\"].replace(',','', regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we remove camera column and change it to new 2 columns ( camera rear and camera front )\n",
    "numerical_columns.pop()\n",
    "numerical_columns.extend([\"Camera_rear\", \"Camera_front\"])\n",
    "# regressor columns are columns that we will fill using another model predictions cause these columns have NAN values ratio > 5-30%\n",
    "# so its better then using mean or median\n",
    "regressor_columns = [\"Android_version\",\"Fast_charging\",\"Camera_rear\", \"Camera_front\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### casting the numerical columns to be float using pandas.to_numeric and removing the nan values except for regressor columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_columns+target_column:\n",
    "    dataset_original[col] = pd.to_numeric(dataset_original[col],errors='coerce')\n",
    "    print(f\"the ratio of nan {col}: {dataset_original[col].isna().sum()*100 / len(dataset_original):.2f}\")\n",
    "    if col not in regressor_columns:\n",
    "        dataset_original[col].fillna(dataset_original[col].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING OUTLIERS AFTER GOT DETECTED USING BOX PLOT WITH OUR KNOWLEDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE THE COLUMNS ORDER IS :\n",
    "# Ram , Battery , Display , Android_version , Inbuilt_memory , Fast_charging , Camera_rear, Camera_front , Price\n",
    "# for example here LOWER BOUND : lowest ram = 2 , lowest battery = 2500 , lowest display = 5.5 ETC\n",
    "# SAME FOR UPPER BOUND                                 \n",
    "lower_bound = [2,2500,5.5,8,16,10,5,2,6950]\n",
    "upper_bound = [16,6500,7.5,14,256,140,80,60,70000]\n",
    "\n",
    "dataset_cleaned_encoded = remove_outliers_with_domain_knowledge(dataset_original, lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILLING NAN VALUES FOR THE REGRESSOR COLUMNS WITH ANOTHER MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note : [\"Android_version\",\"Fast_charging\",\"Camera_rear\", \"Camera_front\"] are the regressor columns cause : \n",
    "#### they have the highest nan values presented , so its better to use advanced technique 'Filling With Regressor Predictions' instead of the regular mean or median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING A DATASET FOR ALL COLUMNS EXCEPT THE TARGET COLUMN ( PRICE ) , ITS BETTER SO WE DONT OVERRIDE OUR ORIGINAL DATASET\n",
    "dataset_for_regressor = dataset_cleaned_encoded[[col for col in dataset_cleaned_encoded.columns if col not in target_column]].copy()\n",
    "# we use random forest regressor to fill nan values here\n",
    "regressor = RandomForestRegressor(n_estimators=100,random_state=random_state)\n",
    "\n",
    "for col in regressor_columns:\n",
    "    # making train dataset using rows that have NOT NULL values in the regressor column \n",
    "    # meaning for example all rows where android_version is not null\n",
    "    train_dataset_for_regressor = dataset_for_regressor[dataset_for_regressor[col].notna()]\n",
    "    # making test dataset using rows that have NULL values in the regressor column\n",
    "    test_dataset_for_regressor = dataset_for_regressor[dataset_for_regressor[col].isna()]\n",
    "\n",
    "    # we drop the column that we want to predict FROM THE DATASET WHERE col is NOT NULL , means all rows where COL was NOT NULL SO WE COULD MAKE PREDICTIONS\n",
    "    X_train = train_dataset_for_regressor.drop(columns=col)\n",
    "    # y is the column that we want to predict FROM THE DATASET WHERE col is NOT NULL , so we could make predictions \n",
    "    y_train = train_dataset_for_regressor[col]\n",
    "    # we drop the column that we want to predict FROM THE DATASET WHERE col is NULL , means all rows where COL was NULL\n",
    "    X_test = test_dataset_for_regressor.drop(columns=col)\n",
    "\n",
    "    regressor.fit(X_train,y_train)\n",
    "    print(f\"prediction for column {col} is done !\")\n",
    "    # FILLING THE COLUMNS WHERE ROW OF col IS NULL WITH THE RESULT OF THE REGRESSOR PRECITION ON THE TEST DATA\n",
    "    dataset_cleaned_encoded.loc[dataset_cleaned_encoded[col].isna(),col] = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW \" DATASET_CLEANED_ENCODED \" READY FOR ALL PREDICTIONS !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plot_visualization(dataset_cleaned_encoded)\n",
    "box_plot_visualization(dataset_cleaned_encoded,'after')\n",
    "heatmap_visualization(dataset_cleaned_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned_encoded.to_csv('dataset_cleaned_encoded.csv',index=False) # saving the cleaned dataset to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned_encoded = pd.read_csv('dataset_cleaned_encoded.csv')\n",
    "X = dataset_cleaned_encoded.drop(target_column,axis=1)\n",
    "y = dataset_cleaned_encoded[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = train_evaluate_with_gridsearch(X, y, models, param_grids)\n",
    "results_df.to_csv('grid_search_results.csv',index=False) # saving the results to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all rows and print the value of each column in the rows\n",
    "for _, row in results_df.iterrows():\n",
    "    for column in results_df.columns:\n",
    "        # rounding the value in case of float number \n",
    "        if isinstance(row[column], float):\n",
    "            print(f\"{column}: {row[column]:.4f}\") # rounding float values\n",
    "        else:\n",
    "            print(f\"{column}: {row[column]}\")\n",
    "    print(\"\\n\")\n",
    "#save_model(best_model,'yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset for making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data =pd.DataFrame({\n",
    "    'Ram': [4,6,6],\n",
    "    'Battery': [5100,5000,6000],\n",
    "    'Display': [6.67,6.71,6.5],\n",
    "    'Android_version': [14,12,14],\n",
    "    'Inbuilt_memory': [128,128,128],\n",
    "    'Fast_charging': [45,15,25],\n",
    "    'Camera_rear': [8,50,50],\n",
    "    'Camera_front': [5,5,13],\n",
    "    'Processor_core': [\"octa core\",\"octa core\",\"octa core\"],\n",
    "    'Company': [\"oppo\",\"poco\",\"samsung\"],\n",
    "    'Processor_name': [\"dimensity\",\"helio\",\"dimensity\"]\n",
    "    })\n",
    "real_prices = [13500,11000,14500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading the downloaded model ' best model '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    if file.endswith('.joblib'):\n",
    "        model_path = os.path.join(os.getcwd(),file)\n",
    "print(model_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making prediction for each best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name,best_model in zip(results_df['model_name'], results_df['model']):\n",
    "    print(f\"\\n{model_name} model predictions :\")\n",
    "    predict_price(input_data, real_prices, best_model, X, lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model predictions :\n",
    "The predicted price of the phone is: 16279.51\n",
    "Difference in price: 2779\n",
    "The predicted price of the phone is: 11132.38\n",
    "Difference in price: 132\n",
    "The predicted price of the phone is: 9468.65\n",
    "Difference in price: -5032\n",
    "\n",
    "---\n",
    "### Poly model predictions :\n",
    "The predicted price of the phone is: 23630.33\n",
    "Difference in price: 10130\n",
    "The predicted price of the phone is: 6950.00\n",
    "Difference in price: -4050\n",
    "The predicted price of the phone is: 15176.92\n",
    "Difference in price: 676\n",
    "\n",
    "---\n",
    "### Lasso model predictions :\n",
    "The predicted price of the phone is: 16252.78\n",
    "Difference in price: 2752\n",
    "The predicted price of the phone is: 11373.38\n",
    "Difference in price: 373\n",
    "The predicted price of the phone is: 9492.55\n",
    "Difference in price: -5008\n",
    "\n",
    "---\n",
    "### Ridge model predictions :\n",
    "The predicted price of the phone is: 18878.68\n",
    "Difference in price: 5378\n",
    "The predicted price of the phone is: 9261.51\n",
    "Difference in price: -1739\n",
    "The predicted price of the phone is: 15923.85\n",
    "Difference in price: 1423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#best_model = joblib.load(model_path)\n",
    "filtered = results_df.loc[(results_df['r2_calculated'] > 0.0) & (results_df['r2_calculated'] < 1.0) ]\n",
    "best_model = results_df.loc[filtered['r2_calculated'].idxmax()] # chosing best model based on the highest r2 score\n",
    "## we dont need to do predictions again , we already made them above\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_name                                                     Lasso\n",
    "\n",
    "---\n",
    "model              (StandardScaler(), Lasso(alpha=20, max_iter=10...\n",
    "\n",
    "---\n",
    "K_cv                                                               8\n",
    "\n",
    "---\n",
    "best_params        {'model__alpha': 20, 'model__max_iter': 10000,...\n",
    "\n",
    "---\n",
    "train_time                                                 14.624879\n",
    "\n",
    "---\n",
    "mse_calculated                                      193589163.421738\n",
    "\n",
    "---\n",
    "mape_calculated                                             0.448297\n",
    "\n",
    "---\n",
    "r2_calculated                                               0.647146\n",
    "\n",
    "---\n",
    "Name: 2, dtype: object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
